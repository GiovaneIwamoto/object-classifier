{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7h72kzqntADH"
      },
      "source": [
        "**CLASSIFIER**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RkDNH2SJLOXN"
      },
      "source": [
        "###CUSTOM ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlvtdHLpeuVp",
        "outputId": "40454607-d5ca-4d18-b7c7-5e321322fb60"
      },
      "outputs": [],
      "source": [
        "!pip install gradient-descent-the-ultimate-optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aDTlpHdeYDHK"
      },
      "outputs": [],
      "source": [
        "# LIBRARIES\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SIq-a8RaJDfD"
      },
      "outputs": [],
      "source": [
        "# GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dOuBMTvzVJJ5"
      },
      "outputs": [],
      "source": [
        "# TRANSFORM\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOeNl-_kJWK8",
        "outputId": "bb78690c-c34d-4716-cc7b-5be8a02a0d70"
      },
      "outputs": [],
      "source": [
        "# CIFAR-100\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JpAdY_1DJsl1"
      },
      "outputs": [],
      "source": [
        "# DATALOADER\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CkZ6gw40LDKE"
      },
      "outputs": [],
      "source": [
        "# 8-CONV LAYER ARCHITECTURE\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 100)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m5TJiw3TNMaL"
      },
      "outputs": [],
      "source": [
        "# MODEL INSTANCE\n",
        "model = CNN().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2yFu6Qp8NN5j"
      },
      "outputs": [],
      "source": [
        "# LOSS / OPTMIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTALmf7kNekb",
        "outputId": "022fb2df-c7b6-4b96-a171-f2b4c795e88d"
      },
      "outputs": [],
      "source": [
        "# MODEL\n",
        "num_epochs = 5\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss Update\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "print(\"Completed training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "65WkRZK72Jsx",
        "outputId": "cf06799c-f40e-4c5c-acc3-1e5034e1f83c"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * total_correct / total_samples\n",
        "print(f\"CUSTOM_MODEL's accuracy on the test data: {accuracy:.2f}%\")\n",
        "\n",
        "# Plot the training loss curve\n",
        "plt.plot(range(num_epochs), train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zdDL2-TmQNXa"
      },
      "outputs": [],
      "source": [
        "# SAVE PARAMS\n",
        "torch.save(model.state_dict(), 'custom_model.pth')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3FFao0lXr8o5"
      },
      "source": [
        "### FINETUNNING - CUSTOM_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nnQq3ucZOPku"
      },
      "outputs": [],
      "source": [
        "# LIBRARIES\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EWZzLPc8unkq"
      },
      "outputs": [],
      "source": [
        "# TRANSFORM\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_file_paths =  '/content/drive/MyDrive/dataset/test'\n",
        "train_file_paths = '/content/drive/MyDrive/dataset/training'\n",
        "val_file_paths = '/content/drive/MyDrive/dataset/validation'\n",
        "\n",
        "test_data = ImageFolder(root=test_file_paths, transform=transform)\n",
        "train_data = ImageFolder(root=train_file_paths, transform=transform)\n",
        "valid_data = ImageFolder(root=val_file_paths, transform=transform)\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_fvpQGbTOcDW"
      },
      "outputs": [],
      "source": [
        "# MODEL\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWVTLHTXKo-p",
        "outputId": "bc349b39-d5f6-4159-b092-cc9ce057c1e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/custom_model.pth'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OysnB8HwsSvW",
        "outputId": "1e84c61c-49e8-4b17-b932-f0a87df535f7"
      },
      "outputs": [],
      "source": [
        "# REPLACE LAST LAYER \n",
        "model.fc_layers = nn.Sequential(\n",
        "    nn.Linear(128 * 4 * 4, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 5)\n",
        ")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b5Ms8HwSsiYS"
      },
      "outputs": [],
      "source": [
        "# LOSS / OPTMIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy-zvPLAtgGW"
      },
      "outputs": [],
      "source": [
        "# FINE-TUNNING\n",
        "num_epochs = 5\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    # Training\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss /= len(train_loader)\n",
        "    valid_loss /= len(valid_loader)\n",
        "\n",
        "    # Store losses\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    # Print losses\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
        "\n",
        "    # Check if validation loss improved\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # Save the model\n",
        "\n",
        "print(\"Completed fine-tuning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z60bvGm31dGn"
      },
      "outputs": [],
      "source": [
        "# LOSS / MATRIX\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "predictions = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        predictions.extend(predicted.tolist())\n",
        "        test_labels.extend(labels.tolist())\n",
        "\n",
        "accuracy = 100 * total_correct / total_samples\n",
        "print(f\"CUSTOM_MODEL's accuracy on the test data: {accuracy:.2f}%\")\n",
        "\n",
        "# Plot the training and validation loss curves\n",
        "plt.plot(range(num_epochs), train_losses, label='Training Loss')\n",
        "plt.plot(range(num_epochs), valid_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Create the confusion matrix\n",
        "predictions = np.array(predictions)\n",
        "test_labels = np.array(test_labels)\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Compute precision, recall, and F1 scores\n",
        "predictions = np.array(predictions)\n",
        "test_labels = np.array(test_labels)\n",
        "report = classification_report(test_loader.dataset.targets, predictions)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsLwppfDEpvx"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'custom_model_fined.pth')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OPL7-dRHEyqc"
      },
      "source": [
        "### FINETUNNING - MOBILENET_V3_SMALL "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP60lcUkFLnH"
      },
      "outputs": [],
      "source": [
        "# LOAD ARCHITECTURE\n",
        "from torchvision.models import mobilenet_v3_small\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9Bx1iMwaxSo"
      },
      "outputs": [],
      "source": [
        "# TRANSFORM\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_file_paths =  '/content/drive/MyDrive/dataset/test'\n",
        "train_file_paths = '/content/drive/MyDrive/dataset/training'\n",
        "val_file_paths = '/content/drive/MyDrive/dataset/validation'\n",
        "\n",
        "test_data = ImageFolder(root=test_file_paths, transform=transform)\n",
        "train_data = ImageFolder(root=train_file_paths, transform=transform)\n",
        "valid_data = ImageFolder(root=val_file_paths, transform=transform)\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYQ9wtn6a1uY"
      },
      "outputs": [],
      "source": [
        "# MODEL\n",
        "model = mobilenet_v3_small(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTT62zTpFSJY"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, len(train_data.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bu4h16lFUcZ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNLPEXxyb7_8"
      },
      "outputs": [],
      "source": [
        "# GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkMnq9kSFn6V"
      },
      "outputs": [],
      "source": [
        "# FINETUNNING\n",
        "num_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_valid_loss:.4f}\")\n",
        "\n",
        "    # Check if validation loss improved\n",
        "    if avg_valid_loss < best_valid_loss:\n",
        "        best_valid_loss = avg_valid_loss\n",
        "        # Save the model\n",
        "\n",
        "print(\"Completed fine-tuning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkOg6dml6KcC"
      },
      "outputs": [],
      "source": [
        "# LOSS / MATRIX\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "predictions = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        predictions.extend(predicted.tolist())\n",
        "        test_labels.extend(labels.tolist())\n",
        "\n",
        "accuracy = 100 * total_correct / total_samples\n",
        "print(f\"MOBILENET_V3_SMALL's accuracy on the test data: {accuracy:.2f}%\")\n",
        "\n",
        "# Plot the training and validation loss curves\n",
        "plt.plot(range(num_epochs), train_losses, label='Training Loss')\n",
        "plt.plot(range(num_epochs), valid_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Create the confusion matrix\n",
        "predictions = np.array(predictions)\n",
        "test_labels = np.array(test_labels)\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Compute precision, recall, and F1 scores\n",
        "predictions = np.array(predictions)\n",
        "test_labels = np.array(test_labels)\n",
        "report = classification_report(test_loader.dataset.targets, predictions)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-MK3tu4J9-V"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'mobilenet_v3_small.pth')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WppAI5FSPCVn"
      },
      "source": [
        "### FINETUNNING - SHUFFLENET_V2_X0_5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25cfIAXWPOGr"
      },
      "outputs": [],
      "source": [
        "# LOAD ARCHITECTURE\n",
        "from torchvision.models import shufflenet_v2_x0_5\n",
        "model = shufflenet_v2_x0_5(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd32cHvJPXgs"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIJErsmqPbn0"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de5R5WBPPeys"
      },
      "outputs": [],
      "source": [
        "# FINETUNNING\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss Update\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Completed fine-tuning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANMCHYyg-8mz"
      },
      "outputs": [],
      "source": [
        "# LOSS / MATRIX\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "predictions = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        predictions.extend(predicted.tolist())\n",
        "        test_labels.extend(labels.tolist())\n",
        "\n",
        "accuracy = 100 * total_correct / total_samples\n",
        "print(f\"SHUFFLENET_V2_X0_5's accuracy on the test data: {accuracy:.2f}%\")\n",
        "\n",
        "if len(train_losses) > num_epochs:\n",
        "    train_losses = train_losses[:num_epochs]\n",
        "elif len(train_losses) < num_epochs:\n",
        "    train_losses.extend([None] * (num_epochs - len(train_losses)))\n",
        "\n",
        "if len(valid_losses) > num_epochs:\n",
        "    valid_losses = valid_losses[:num_epochs]\n",
        "elif len(valid_losses) < num_epochs:\n",
        "    valid_losses.extend([None] * (num_epochs - len(valid_losses)))\n",
        "\n",
        "# Plot the training and validation loss curves\n",
        "plt.plot(range(num_epochs), train_losses, label='Training Loss')\n",
        "plt.plot(range(num_epochs), valid_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Create the confusion matrix\n",
        "predictions = np.array(predictions)\n",
        "test_labels = np.array(test_labels)\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Compute precision, recall, and F1 scores\n",
        "predictions = np.array(predictions)\n",
        "test_labels = np.array(test_labels)\n",
        "report = classification_report(test_loader.dataset.targets, predictions)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSJF83QpPhGc"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'shufflenet_v2_x0_5.pth')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WzycgiPmT4G7"
      },
      "source": [
        "### FINETUNNING - SQUEEZENET1_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2ayN-eyUC2U"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import squeezenet1_0\n",
        "model = squeezenet1_0(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkzXlkn4UTa5"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "model.classifier._modules['1'] = nn.Conv2d(512, num_classes, kernel_size=(1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6zs7eSsUdjZ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2LVA1KRUgCG"
      },
      "outputs": [],
      "source": [
        "# FINETUNNING\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss Update\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Completed fine-tuning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqhK-3gc_LAP"
      },
      "outputs": [],
      "source": [
        "# LOSS / MATRIX\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "predictions = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        predictions.extend(predicted.tolist())\n",
        "        test_labels.extend(labels.tolist())\n",
        "\n",
        "accuracy = 100 * total_correct / total_samples\n",
        "print(f\"SQUEEZENET1_0's accuracy on the test data: {accuracy:.2f}%\")\n",
        "\n",
        "# Plot the training and validation loss curves\n",
        "plt.plot(range(num_epochs), train_losses, label='Training Loss')\n",
        "plt.plot(range(num_epochs), valid_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Create the confusion matrix\n",
        "predictions = np.array(predictions)\n",
        "test_labels = np.array(test_labels)\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Compute precision, recall, and F1 scores\n",
        "predictions = np.array(predictions)\n",
        "test_labels = np.array(test_labels)\n",
        "report = classification_report(test_loader.dataset.targets, predictions)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SJOBUnKU0UR"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'squeezenet1_0.pth')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
